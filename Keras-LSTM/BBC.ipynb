{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0d708b92321763157f37de519d81887a0eb2dbc14183f20cda4a0707ff0a2fa51",
   "display_name": "Python 3.9.4 64-bit ('my_nlp_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np \n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from time import time \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.layers import LSTM, Dropout, Embedding\n",
    "#from tensorflow.keras.layers import Bidirectional \n",
    "from sklearn.metrics import confusion_matrix, f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번 블럭\n",
    "# 파라미터 설정\n",
    "MY_VOCAB = 5000   # 내가 사용할 단어 수, 인기도 기준\n",
    "MY_EMBED = 64     # 임베딩 차원\n",
    "MY_HIDDEN = 100   # RNN 셀의 규모\n",
    "MY_LEN = 200      # 기사의 길이\n",
    "\n",
    "MY_SPLIT = 0.8    # 학습용 데이터의 비율\n",
    "MY_SAMPLE = 121   # 샘플 기사\n",
    "MY_EPOCH = 10    # 반복 학습 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "제외어:\n",
      "{'the', 'hers', 'ourselves', \"shan't\", 'when', 'should', 'is', \"it's\", 'to', 'their', 'with', \"you'd\", 'them', 'there', 'who', 'm', 'if', 'having', 'after', 'have', 'below', 'mustn', \"won't\", 'very', 'before', 'these', 'further', \"mightn't\", 'themselves', 'whom', 't', 'between', 'itself', 'again', \"you're\", 'nor', 'only', 'your', 'o', 'or', \"doesn't\", 'not', 'some', 'any', 'myself', 'a', 'all', 'such', \"you'll\", 'too', \"isn't\", 'doing', 'ain', 'will', \"didn't\", 'yourself', 'didn', 'what', 'same', 'it', \"that'll\", 'does', 'herself', 'she', 'did', 're', 'theirs', 'am', \"haven't\", 'been', 'on', 'which', 'an', 'during', 'both', 'into', 'once', 'had', 'here', 'than', \"weren't\", \"needn't\", 'above', 'they', 'mightn', \"wasn't\", 'of', 'll', 'was', 'haven', 'doesn', \"mustn't\", 'over', 'her', 'where', 'most', 'at', 'under', 've', 'for', \"aren't\", 'has', 'this', 'won', 'out', 'other', 'hasn', 'why', \"hadn't\", 'how', 'so', 'were', 'then', 'now', 'from', 'while', 'few', 'don', 'himself', 'those', 'own', 'yours', 'through', 'off', 'more', 'by', 'weren', 'do', 'shan', 'because', 'up', 's', 'and', \"you've\", 'down', 'no', 'being', \"don't\", 'y', \"should've\", 'd', 'can', 'isn', 'in', \"wouldn't\", 'aren', 'we', 'his', 'about', \"hasn't\", 'me', \"she's\", 'needn', 'its', 'yourselves', 'shouldn', 'wasn', 'just', 'be', 'as', 'my', 'against', 'ours', 'ma', 'hadn', 'but', 'couldn', \"couldn't\", 'each', 'he', 'are', 'wouldn', 'i', 'him', 'that', 'our', \"shouldn't\", 'you', 'until'}\n",
      "제외어 갯수: 179\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#제외어 (Stopword) 설정 \n",
    "\n",
    "nltk.download('stopwords')\n",
    "MY_STOP = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "print('제외어:')\n",
    "print(MY_STOP)\n",
    "print('제외어 갯수:', len(MY_STOP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 4번 블럭\n",
    "# 데이터 보관 창고\n",
    "original = []\n",
    "articles = []\n",
    "labels = []\n",
    "\n",
    "print(type(original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['category', 'text']\n",
      "처리한 기사 숫자: 2225\n"
     ]
    }
   ],
   "source": [
    "with open('bbc-text.csv', 'r') as file:\n",
    "    \n",
    "\n",
    "    # 컬럼 이름 읽기\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    print(header)\n",
    "\n",
    "    # 기사 한줄씩 처리\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        original.append(row[1])\n",
    "\n",
    "        # 제외어 삭제하기\n",
    "        news = row[1]\n",
    "        #print('전:', news)\n",
    "        for word in MY_STOP:\n",
    "            mask = ' ' + word + ' '\n",
    "            news = news.replace(mask, ' ')\n",
    "        #print('후:', news)\n",
    "\n",
    "        articles.append(news)\n",
    "print('처리한 기사 숫자:', len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "샘플 기사 원본\njack cunningham to stand down veteran labour mp and former cabinet minister jack cunningham has said he will stand down at the next election.  one of the few blair-era ministers to serve under jim callaghan  he was given the agriculture portfolio when labour regained power in 1997. mr cunningham went on to become tony blair s  cabinet enforcer . he has represented the constituency now known as copeland since 1970. mr blair said he was a  huge figure  in labour and a  valued  personal friend .  during labour s long period in opposition  mr cunningham held a number of shadow roles including foreign affairs  the environment and as trade spokesman. as agriculture minister he caused controversy when he decided to ban beef on the bone in the wake of fears over bse. he quit the government in 1999 and in recent years has served as the chairman of the all-party committee on lords reform and has been a loyal supporter of the government from the backbenches.\npolitics\n<class 'str'>\n총 단어 수: 166\n"
     ]
    }
   ],
   "source": [
    "#샘플기사 원본 출력\n",
    "\n",
    "print('샘플 기사 원본')\n",
    "print(original[MY_SAMPLE])\n",
    "print(labels[MY_SAMPLE])\n",
    "print(type(original[MY_SAMPLE]))\n",
    "print('총 단어 수:', len(original[MY_SAMPLE].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "제외어 삭제본\njack cunningham stand veteran labour mp former cabinet minister jack cunningham said stand next election.  one blair-era ministers serve jim callaghan  given agriculture portfolio labour regained power 1997. mr cunningham went become tony blair  cabinet enforcer . represented constituency known copeland since 1970. mr blair said  huge figure  labour  valued  personal friend .  labour long period opposition  mr cunningham held number shadow roles including foreign affairs  environment trade spokesman. agriculture minister caused controversy decided ban beef bone wake fears bse. quit government 1999 recent years served chairman all-party committee lords reform loyal supporter government backbenches.\n총 단어 수: 95\n<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#7번 블럭\n",
    "#제외어 삭제 기사 출력\n",
    "print('제외어 삭제본')\n",
    "print(articles[MY_SAMPLE])\n",
    "print('총 단어 수:', len(articles[MY_SAMPLE].split()))\n",
    "print(type(articles[MY_SAMPLE]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원본:  goldsmith:  i was not leant on  the attorney general has again denied being  leant on  by downing street to make the legal case for invading iraq.  claims a written answer on the legality of the war was drafted by downing street were  wholly unfounded   he insisted during stormy lords exchanges. lord goldsmith said the answer represented his  genuinely held independent view  the war was legal. the text was released on the eve of a crucial commons vote in which mps backed the invasion of iraq. many labour mps have since indicated that the attorney general s answer played a pivotal role in their willingness to back the conflict. the government has resisted calls to publish the full advice  saying such papers are always kept confidential. in the house of lords  the attorney general faced a call by former tory lord chancellor lord mackay to now publish the  full text  of the advice - the suggestion was rejected. another peer meanwhile  lord skidelsky  said not to publish the full legal opinion would  strengthen the suspicion that the the original text was doctored for public consumption  in exactly the same way as the notorious intelligence dossier on weapons of mass destruction .  last week lord goldsmith said in a statement:  i was fully involved throughout the drafting process and personally finalised  and of course approved  the answer.  he said the answer had been prepared in his office with the involvement of solicitor general harriet harman  two of his own officials  three foreign office officials  a qc  christopher greenwood and the then lord chancellor  lord irvine of lairg.  no other minister or official was involved in any way.   as i have always made clear  i set out in the answer my own genuinely held  independent view that military action was lawful under the existing (un) security council resolutions   he said.  the answer did not purport to be a summary of my confidential legal advice to government.  former foreign secretary robin cook said lord goldsmith s admission that his parliamentary answer was not a summary of his legal opinion suggested parliament may have been misled.  the attorney general may never have presented his answer as a summary  but others certainly did   he said.  what is clear from his statement today is that he does not believe that it was a full  accurate summary of his formal opinion.   tony blair has dismissed questions about the attorney general s advice  and said his parliamentary statement had been a  fair summary  of his opinion.  that s what he [lord goldsmith] said and that s what i say. he has dealt with this time and time and time again   mr blair told his monthly news conference in downing street. he refused to answer further questions on the issue. on the question of whether such papers have always been kept confidential  tory mp michael mates  who is a member of the commons intelligence and security committee and was part of the butler inquiry  told the bbc:  that  as a general rule  is right  but it s not an absolute rule.  he said there had been other occasions when advice had been published  most recently regarding prince charles s marriage plans. the government could not pick and choose when to use the convention  he said. mr mates added:  this may be one of those special occasions... when it would be in the public interest to see the advice which the attorney general gave to the prime minister.  a book published by philippe sands qc  a member of cherie blair s matrix chambers says lord goldsmith warned tony blair on 7 march 2003 that the iraq war could be illegal without a second un resolution sanctioning military action. a short statement about lord goldsmith s position presented in a written parliamentary answer on 17 march 2003 - just before a crucial commons vote on the military action - did not suggest this.\n처리:  goldsmith:  leant  attorney general denied  leant  downing street make legal case invading iraq.  claims written answer legality war drafted downing street  wholly unfounded   insisted stormy lords exchanges. lord goldsmith said answer represented  genuinely held independent view  war legal. text released eve crucial commons vote mps backed invasion iraq. many labour mps since indicated attorney general answer played pivotal role willingness back conflict. government resisted calls publish full advice  saying papers always kept confidential. house lords  attorney general faced call former tory lord chancellor lord mackay publish  full text  advice - suggestion rejected. another peer meanwhile  lord skidelsky  said publish full legal opinion would  strengthen suspicion the original text doctored public consumption  exactly way notorious intelligence dossier weapons mass destruction .  last week lord goldsmith said statement:  fully involved throughout drafting process personally finalised  course approved  answer.  said answer prepared office involvement solicitor general harriet harman  two officials  three foreign office officials  qc  christopher greenwood lord chancellor  lord irvine lairg.  minister official involved way.   always made clear  set answer genuinely held  independent view military action lawful existing (un) security council resolutions   said.  answer purport summary confidential legal advice government.  former foreign secretary robin cook said lord goldsmith admission parliamentary answer summary legal opinion suggested parliament may misled.  attorney general may never presented answer summary  others certainly   said.  clear statement today believe full  accurate summary formal opinion.   tony blair dismissed questions attorney general advice  said parliamentary statement  fair summary  opinion.  [lord goldsmith] said say. dealt time time time   mr blair told monthly news conference downing street. refused answer questions issue. question whether papers always kept confidential  tory mp michael mates  member commons intelligence security committee part butler inquiry  told bbc:   general rule  right  absolute rule.  said occasions advice published  recently regarding prince charles marriage plans. government could pick choose use convention  said. mr mates added:  may one special occasions... would public interest see advice attorney general gave prime minister.  book published philippe sands qc  member cherie blair matrix chambers says lord goldsmith warned tony blair 7 march 2003 iraq war could illegal without second un resolution sanctioning military action. short statement lord goldsmith position presented written parliamentary answer 17 march 2003 - crucial commons vote military action - suggest this.\n원본:  profits slide at india s dr reddy profits at indian drugmaker dr reddy s fell 93% as research costs rose and sales flagged.  the firm said its profits were 40m rupees ($915 000; 짙486 000) for the three months to december on sales which fell 8% to 4.7bn rupees. dr reddy s has built its reputation on producing generic versions of big-name pharmaceutical products. but competition has intensified and the firm and the company is short on new product launches. the most recent was the annoucement in december 2000 that it had won exclusive marketing rights for a generic version of the famous anti-depressant prozac from its maker  eli lilly. it also lost a key court case in march 2004  banning it from selling a version of pfizer s popular hypertension drug norvasc in the us. research and development of new drugs is continuing apace  with r&d spending rising 37% to 705m rupees - a key cause of the decrease in profits alongside the fall in sales. patents on a number of well-known products are due to run out in the near future  representing an opportunity for dr reddy  whose shares are listed in new york  and other indian generics manufacturers.  sales in dr reddy s generics business fell 8.6% to 966m rupees. another staple of the the firm s business  the sale of ingredients for drugs  also performed poorly. sales were down more than 25% from the previous year to 1.4bn rupees in the face of strong competition both at home  and in the us and europe. dr reddy s indian competitors are gathering strength although they too face heavy competitive pressures.\n처리:  profits slide india dr reddy profits indian drugmaker dr reddy fell 93% research costs rose sales flagged.  firm said profits 40m rupees ($915 000; 짙486 000) three months december sales fell 8% 4.7bn rupees. dr reddy built reputation producing generic versions big-name pharmaceutical products. competition intensified firm company short new product launches. recent annoucement december 2000 exclusive marketing rights generic version famous anti-depressant prozac maker  eli lilly. also lost key court case march 2004  banning selling version pfizer popular hypertension drug norvasc us. research development new drugs continuing apace  r&d spending rising 37% 705m rupees - key cause decrease profits alongside fall sales. patents number well-known products due run near future  representing opportunity dr reddy  whose shares listed new york  indian generics manufacturers.  sales dr reddy generics business fell 8.6% 966m rupees. another staple the firm business  sale ingredients drugs  also performed poorly. sales 25% previous year 1.4bn rupees face strong competition home  us europe. dr reddy indian competitors gathering strength although face heavy competitive pressures.\n원본:  uefa approves fake grass uefa says it will allow european matches to be played on artificial pitches from the start of next season.  european football s governing body made the decision at a meeting of its executive committee on wednesday. uefa explained that the move  follows comprehensive studies into the sporting and medical aspects of using artificial playing surfaces.  they can be used subject to complying with uefa quality criteria but there use will not be made obligatory. luton  preston  queens park rangers and other clubs used to have plastic pitches during the the 1980s but  after a two-year study  uefa insists the surfaces have moved on. international matches can also be played on such pitches  although games at major tournaments have to be contested on grass. uefa spokesman rob faulkner said:  people in england have bad memories of the artificial pitches of luton and qpr in the 1980s  but the latest generation are completely different and are much more like grass.  we have sanctioned its use from the start of next season but only as long as it is the latest generation of artificial turf and meets a whole series of standards.  several leading clubs from scandinavia  russia and eastern europe - especially those who only play champions league or uefa cup matches in winter - are now expected to instal artificial pitches.\n처리:  uefa approves fake grass uefa says allow european matches played artificial pitches start next season.  european football governing body made decision meeting executive committee wednesday. uefa explained move  follows comprehensive studies sporting medical aspects using artificial playing surfaces.  used subject complying uefa quality criteria use made obligatory. luton  preston  queens park rangers clubs used plastic pitches the 1980s  two-year study  uefa insists surfaces moved on. international matches also played pitches  although games major tournaments contested grass. uefa spokesman rob faulkner said:  people england bad memories artificial pitches luton qpr 1980s  latest generation completely different much like grass.  sanctioned use start next season long latest generation artificial turf meets whole series standards.  several leading clubs scandinavia  russia eastern europe - especially play champions league uefa cup matches winter - expected instal artificial pitches.\n원본:  france set for new da vinci novel french booksellers are braced for a rush of interest after another book from the author of the da vinci code is translated into french.  angels and demons  by us author dan brown  will go on sale on wednesday. the da vinci code is set in paris - including the louvre - and has sold around one million copies in france. the main character  robert langdon  also appears in angels and demons. the da vinci code is being made into a film starring tom hanks. angels and demons was written before the da vinci code  which has sold more than 20 million copies worldwide  and been translated into more than 40 languages  since it was released in 2003. angels and demons is set mainly in rome as symbologist robert langdon follows a 400-year-old trail to try to uncover a plot by an ancient brotherhood  the illuminati  to blow up the vatican.  the novel deals with moral issues such as the debate between science and religion and also seeks to uncover some of the mysteries surrounding the pope. on his website  brown wrote:  i think the reason angels and demons is raising eyebrows right now is that it opens some vatican closets most people don t even know exist.  but i think most people understand that an organisation as old and powerful as the vatican could not possibly have risen to power without acquiring a few skeletons in their closets.  such is the success of the da vinci code in france  special tours have been organised to trace langdon s footsteps  including the the louvre museum and the saint sulpice church. the louvre has also given permission for parts of the film version to be shot in the museum. the film  to be directed by a beautiful mind s ron howard  is due to start filming at the paris museum in may and stars hanks alongside french actress audrey tautou.\n처리:  france set new da vinci novel french booksellers braced rush interest another book author da vinci code translated french.  angels demons  us author dan brown  go sale wednesday. da vinci code set paris - including louvre - sold around one million copies france. main character  robert langdon  also appears angels demons. da vinci code made film starring tom hanks. angels demons written da vinci code  sold 20 million copies worldwide  translated 40 languages  since released 2003. angels demons set mainly rome symbologist robert langdon follows 400-year-old trail try uncover plot ancient brotherhood  illuminati  blow vatican.  novel deals moral issues debate science religion also seeks uncover mysteries surrounding pope. website  brown wrote:  think reason angels demons raising eyebrows right opens vatican closets people even know exist.  think people understand organisation old powerful vatican could possibly risen power without acquiring skeletons closets.  success da vinci code france  special tours organised trace langdon footsteps  including the louvre museum saint sulpice church. louvre also given permission parts film version shot museum. film  directed beautiful mind ron howard  due start filming paris museum may stars hanks alongside french actress audrey tautou.\n원본:  brookside creator s channel 4 bid the creator of defunct tv soap brookside has written to the culture minister to offer to buy channel 4.  phil redmond  now chairman of mersey tv  told tessa jowell he would run it with its current remit intact for the next 10 years. but media watchdog ofcom has said the the commercially funded public service broadcaster will not be privatised. a spokesman for the department for culture  media and sport said there were no plans to sell the channel.  he added that primary legislation would be required for the station to be sold off  which the government was not intending to introduce. brookside was axed in 2003 after its ratings slumped from a peak of seven million to just 1.5 million. redmond also brought teen soap hollyoaks to channel 4 and created grange hill  the school-based drama serial which was first broadcast on bbc one in 1978. he was awarded the cbe for services to drama earlier this year.\n처리:  brookside creator channel 4 bid creator defunct tv soap brookside written culture minister offer buy channel 4.  phil redmond  chairman mersey tv  told tessa jowell would run current remit intact next 10 years. media watchdog ofcom said the commercially funded public service broadcaster privatised. spokesman department culture  media sport said plans sell channel.  added primary legislation would required station sold  government intending introduce. brookside axed 2003 ratings slumped peak seven million 1.5 million. redmond also brought teen soap hollyoaks channel 4 created grange hill  school-based drama serial first broadcast bbc one 1978. awarded cbe services drama earlier year.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(articles)):\n",
    "    if ' the ' in articles[i]:\n",
    "        print('원본: ', original[i])\n",
    "        print('처리: ', articles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "28612, 'deodorant': 28613, 'mister': 28614, '449': 28615, 'owing': 28616, 'swelled': 28617, 'babacan': 28618, 'accommodated': 28619, 'substitutions': 28620, 'sniffer': 28621, 'guti': 28622, 'ser': 28623, 'nihat': 28624, 'kahveci': 28625, 'albacete': 28626, 'separatist': 28627, 'accosiation': 28628, 'shamil': 28629, 'fabrication': 28630, 'raps': 28631, 'overcharged': 28632, 'departing': 28633, 'hassall': 28634, 'illston': 28635, 'evidentiary': 28636, 'admissibility': 28637, 'vima': 28638, 'childish': 28639, '짙49m': 28640, 'paion': 28641, 'clotting': 28642, 'cursed': 28643, 'werewolf': 28644, 'ranger': 28645, 'cheerleader': 28646, 'elise': 28647, 'philandering': 28648, 'madea': 28649, 'powerpoint': 28650, 'telemedicine': 28651, 'personalized': 28652, 'hatched': 28653, 'designate': 28654, 'prassana': 28655, 'rambathla': 28656, 'choke': 28657, 'foolproof': 28658, 'starbuck': 28659, 'intermittency': 28660, 'pctvts': 28661, 'insect': 28662, 'postulating': 28663, 'numbered': 28664, 'ecosystems': 28665, 'equilibrium': 28666, 'continuum': 28667, 'patrolling': 28668, 'distinguishing': 28669, 'discernible': 28670, 'begging': 28671, '39bn': 28672, 'luring': 28673, '069': 28674, 'tractor': 28675, 'mahindra': 28676, '874': 28677, 'swindle': 28678, 'hustlers': 28679, 'illarionov': 28680, 'iliaronov': 28681, 'privatized': 28682, 'nationalise': 28683, 'webs': 28684, 'qualitatively': 28685, 'cel': 28686, 'ocarina': 28687, 'adventuring': 28688, 'warioware': 28689, 'tarkovsky': 28690, 'mutation': 28691, 'hideo': 28692, 'kojima': 28693, 'tecmo': 28694, 'ninja': 28695, 'retooled': 28696, 'gutful': 28697, 'modifications': 28698, 'ultimitely': 28699, 'starters': 28700, 'adaptibility': 28701, '6in': 28702, 'rabbit': 28703, 'blanket': 28704, 'quarry': 28705, 'farquhar': 28706, 'prevaricated': 28707, 'otter': 28708, 'badger': 28709, 'timor': 28710, 'consuls': 28711, 'bordeaux': 28712, 'oporto': 28713, 'redeployed': 28714, 'pretoria': 28715, 'mbabane': 28716, 'kabul': 28717, 'pyongyang': 28718, '짙86m': 28719, 'retirements': 28720, 'golfers': 28721, 'fairway': 28722, 'responsibilty': 28723, 'inconvenienced': 28724, 'tenets': 28725, 'illiberalism': 28726, 'demean': 28727, 'italaudit': 28728, 'touche': 28729, 'consob': 28730, 'priceless': 28731, 'slaughtered': 28732, 'hatful': 28733, 'prioritises': 28734, 'solidifies': 28735, 'calming': 28736, 'sprain': 28737, 'frampton': 28738, 'deon': 28739, 'saux': 28740, 'svensson': 28741, 'oakley': 28742, 'carrot': 28743, 'sodje': 28744, 'torquay': 28745, 'eventful': 28746, 'docherty': 28747, 'solent': 28748, 'millmoor': 28749, 'antwerp': 28750, 'willems': 28751, 'biotech': 28752, 'spray': 28753, 'satifex': 28754, 'tetrahydrocannabinol': 28755, 'cannabidiol': 28756, 'arthritis': 28757, 'auxerre': 28758, 'tenders': 28759, 'regretted': 28760, 'yuri': 28761, 'wrested': 28762, 'kulakov': 28763, 'barrick': 28764, 'cfa': 28765, 'existent': 28766, 'ephraim': 28767, 'unearned': 28768, 'fidelis': 28769, 'nanga': 28770, 'yaounde': 28771, 'exemplary': 28772, 'meted': 28773, 'densely': 28774, 'mercantile': 28775, 'nymex': 28776, '113th': 28777, 'regulating': 28778, 'underage': 28779, '2am': 28780, 'exacerbate': 28781, 'bandaid': 28782, '2340': 28783, 'sportscar': 28784, 'mediobanca': 28785, '775m': 28786, '짙788': 28787, '439': 28788, '짙507': 28789, '596': 28790, 'munchies': 28791, 'narrows': 28792, 'dales': 28793, 'prognosis': 28794, 'sorrows': 28795, 'soybean': 28796, 'peptides': 28797, 'shinnama': 28798, '291m': 28799, '짙154m': 28800, 'suntory': 28801, 'shochu': 28802, 'distilled': 28803, 'kirin': 28804, 'energetically': 28805, 'strands': 28806, 'carradine': 28807, 'mirroring': 28808, '짙600m': 28809, '639m': 28810, '284m': 28811, 'catwoman': 28812, '09bn': 28813, 'bertelsmann': 28814, 'cidatel': 28815, 'infomercial': 28816, 'pennsylvania': 28817, 'syndicated': 28818, 'rantings': 28819, '짙950': 28820, 'barometer': 28821, 'stategist': 28822, 'downshifting': 28823, 'cherney': 28824, 'learns': 28825, 'gainful': 28826, 'jog': 28827, '10cm': 28828, '130cm': 28829, '2kg': 28830, 'beckoned': 28831, 'entertainer': 28832, 'amusement': 28833, 'improbability': 28834, 'inefficiencies': 28835, 'burgeoned': 28836, 'hidebound': 28837, 'overspending': 28838, 'misrepresenting': 28839, 'minghella': 28840, 'bonham': 28841, 'bifa': 28842, 'earring': 28843, 'birthistle': 28844, 'hickox': 28845, 'pavlikowsky': 28846, '520': 28847, 'monterey': 28848, 'auctioneers': 28849, 'shefrin': 28850, 'holly': 28851, 'frankin': 28852, 'bloopers': 28853, '짙775': 28854, 'pester': 28855, 'stump': 28856, 'individuality': 28857, 'unlicensed': 28858, 'minicabs': 28859, 'marson': 28860, 'hypertags': 28861, 'bookmarks': 28862, '1000': 28863, 'informally': 28864, 'suranga': 28865, 'promiscuous': 28866, '785m': 28867, 'unfavourably': 28868, '784m': 28869, '374m': 28870, 'rabb': 28871, 'jeep': 28872, '506m': 28873, '142bn': 28874, 'schrempp': 28875, '475m': 28876, 'daimerchrysler': 28877, 'thefirst': 28878, 'abortive': 28879, 'loneliness': 28880, 'wilt': 28881, 'unquestionable': 28882, 'daley': 28883, 'gnaw': 28884, 'emphatically': 28885, 'pronouncement': 28886, 'kurosawa': 28887, 'accura': 28888, 'trans': 28889, 'x5': 28890, 'x3': 28891, 'arlington': 28892, '325m': 28893, 'envisioned': 28894, 'col': 28895, 'schnaible': 28896, 'permissive': 28897, 'symptomatic': 28898, 'timers': 28899, 'hotbot': 28900, 'altavista': 28901, 'urs': 28902, 'trumpeted': 28903, 'truer': 28904, 'financials': 28905, 'muscling': 28906, 'outflank': 28907, 'unlocking': 28908, 'parmjit': 28909, 'coruna': 28910, 'jorge': 28911, 'andrade': 28912, 'depor': 28913, 'irureta': 28914, 'defeatism': 28915, 'beaudoin': 28916, 'montreal': 28917, 'snowmobile': 28918, 'seating': 28919, '135': 28920, 'reshape': 28921, 'consensual': 28922, 'succour': 28923, 'sas': 28924, 'affirmative': 28925, '932': 28926, 'quashes': 28927, 'eon': 28928, 'teva': 28929, 'chemically': 28930, 'drugmakers': 28931, 'kuhlhoff': 28932, 'assassinated': 28933, 'rafik': 28934, '08': 28935, 'blom': 28936, '642': 28937, 'redeveloped': 28938, 'shudders': 28939, 'megane': 28940, 'clio': 28941, 'logan': 28942, 'perjury': 28943, 'perverting': 28944, 'elapsed': 28945, 'prostitute': 28946, 'tebbit': 28947, 'perjurer': 28948, 'forgiving': 28949, 'newby': 28950, 'hefce': 28951, 'neighbourhoods': 28952, 'postgraduate': 28953, 'aspire': 28954, 'howells': 28955, 'deferred': 28956, '387m': 28957, '짙206m': 28958, '111m': 28959, 'arpey': 28960, '761m': 28961, 'roms': 28962, 'threatend': 28963, 'poisonous': 28964, 'dejected': 28965, 'peformance': 28966, 'magath': 28967, 'thereabouts': 28968, 'linvoy': 28969, 'flutter': 28970, 'punch': 28971, 'teased': 28972, 'patrik': 28973, 'aiyegbeni': 28974, 'reflexes': 28975, 'scraps': 28976, 'fireside': 28977, 'edgar': 28978, 'poe': 28979, 'bierce': 28980, 'indexes': 28981, 'reigns': 28982, 'encarta': 28983, 'encyclopaedia': 28984, 'equations': 28985, 'algorithmic': 28986, 'equalisers': 28987, 'avid': 28988, 'attributable': 28989, 'kos': 28990, 'instapundit': 28991, 'educations': 28992, 'congruent': 28993, 'comerica': 28994, 'littman': 28995, 'whittle': 28996, 'utilisation': 28997, 'cary': 28998, 'lean': 28999, 'wieting': 29000, 'bubbling': 29001, 'joysticks': 29002, 'gamepads': 29003, 'iowa': 29004, 'virtues': 29005, 'vices': 29006, 'astonished': 29007, 'forefathers': 29008, 'frontiers': 29009, 'uruguayan': 29010, 'newsgaming': 29011, 'cambiemos': 29012, 'matshushita': 29013, 'maloti': 29014, 'unceremoniously': 29015, 'tw': 29016, 'vogue': 29017, 'bangaldesh': 29018, 'mafeca': 29019, 'agoa': 29020, 'upkeep': 29021, 'jeffries': 29022, 'fugill': 29023, 'lea': 29024, 'mph': 29025, 'pease': 29026, 'pottage': 29027, 'monkhouse': 29028, 'tuxford': 29029, 'alcorn': 29030, 'osx': 29031, 'wojcicki': 29032, 'extracts': 29033, 'bibliographies': 29034, 'scholarly': 29035, 'reg': 29036, 'leclerc': 29037, 'wilkin': 29038, 'bruer': 29039, 'frattini': 29040, 'contravene': 29041, 'stacks': 29042, 'damndest': 29043, 'humming': 29044, 'muso': 29045, 'posteriors': 29046, 'disused': 29047, 'rename': 29048, 'raiding': 29049, 'arresting': 29050, 'contravening': 29051, 'abatement': 29052, 'domino': 29053, 'smog': 29054, 'sebadoh': 29055, 'tet': 29056, 'swallows': 29057, '353': 29058, '짙183': 29059, 'hoegaarden': 29060, 'staropramen': 29061, 'leuven': 29062, 'klin': 29063, 'ivanovo': 29064, 'saransk': 29065, 'volzhsky': 29066, 'omsk': 29067, 'novocheboksarsk': 29068, 'chernigov': 29069, 'nikolaev': 29070, 'kharkov': 29071, '273': 29072, 'harvest': 29073, 'middlemen': 29074, 'persistence': 29075, 'scattergun': 29076, 'vies': 29077, 'supernanny': 29078, 'lucerne': 29079, 'borat': 29080, 'dancefloor': 29081, 'varirty': 29082, 'flashmob': 29083, 'agatha': 29084, 'bunuel': 29085, 'crusoe': 29086, 'macduff': 29087, 'orson': 29088, 'welles': 29089, 'omni': 29090, 'thrower': 29091, 'curling': 29092, 'karate': 29093, 'robustness': 29094, 'sails': 29095, 'berson': 29096, 'bard': 29097, 'tidy': 29098, 'gaobot': 29099, 'randex': 29100, 'variations': 29101, 'mosquito': 29102, 'jaschen': 29103, 'sasser': 29104, 'licencing': 29105, 'administrations': 29106, 'harmonisation': 29107, 'theriault': 29108, 'ieee': 29109, 'drags': 29110, 'tendinitis': 29111, '97p': 29112, '44p': 29113, '짙258': 29114, 'everasia': 29115, 'iog': 29116, 'ironhorse': 29117, 'suba': 29118, 'luhais': 29119, 'eighties': 29120, 'pervez': 29121, 'pheromones': 29122, 'daylight': 29123, 'pheremones': 29124, 'exerting': 29125, 'kofi': 29126, 'annan': 29127, 'inserting': 29128, 'solves': 29129, 'idefence': 29130, 'pls': 29131, 'm3u': 29132, '짙162': 29133, '086': 29134, 'portfolios': 29135, 'emergencies': 29136, 'intensify': 29137, 'unafraid': 29138, 'invasions': 29139, 'censure': 29140, 'kosovo': 29141, 'spurns': 29142, 'airy': 29143, 'libertarians': 29144, 'liberati': 29145, 'amalgam': 29146, 'glitterati': 29147, 'courted': 29148, 'reclassification': 29149, 'divorcee': 29150, 'casually': 29151, 'schooled': 29152, 'shadowing': 29153, 'heckled': 29154, 'conciliator': 29155, 'sadie': 29156, 'labrador': 29157, 'curly': 29158, 'coated': 29159, 'retriever': 29160, '1650': 29161, '3280': 29162, '8923': 29163, 'tacitly': 29164, 'classifications': 29165, 'chainsaw': 29166, 'responsibly': 29167, 'realities': 29168, '75s': 29169, 'reintroducing': 29170, 'dental': 29171, 'polluter': 29172, '짙630': 29173, 'fledging': 29174, 'brushes': 29175, 'gutted': 29176, 'greed': 29177, 'sixways': 29178, 'pieters': 29179, 'rasmussen': 29180, 'windo': 29181, 'horsman': 29182, 'gillies': 29183, 'fortey': 29184, 'vaili': 29185, 'trueman': 29186, 'redpath': 29187, 'roddam': 29188, 'schofield': 29189, 'caillet': 29190, 'bozzi': 29191, 'coutts': 29192, 'anglesea': 29193, 'lund': 29194, 'martens': 29195, 'acute': 29196, 'amended': 29197, 'expunged': 29198, 'drafts': 29199, 'tendering': 29200, 'mickey': 29201, '253m': 29202, '짙175m': 29203, '328m': 29204, 'arabian': 29205, 'walid': 29206, '45pm': 29207, 'suspense': 29208, 'banquet': 29209, 'dorothy': 29210, 'herrick': 29211, 'overheard': 29212, 'skolsky': 29213, '1939': 29214, 'statuettes': 29215, 'statues': 29216, 'sr': 29217, 'admittance': 29218, 'receipt': 29219, 'jolson': 29220, 'humphrey': 29221, 'bogart': 29222, 'casablanca': 29223, '1944': 29224, 'fonda': 29225, 'hitchcock': 29226, 'glitz': 29227, 'demure': 29228, 'dresses': 29229, 'sacheen': 29230, 'littlefeather': 29231, 'cruz': 29232, 'streaked': 29233, 'bounded': 29234, 'cavalcade': 29235, 'palms': 29236, 'immune': 29237, 'pointsec': 29238, 'backup': 29239, 'oslo': 29240, 'danes': 29241, 'norwegians': 29242, 'swedes': 29243, 'retrofone': 29244, 'muggings': 29245, 'snatch': 29246, 'harp': 29247, 'hamster': 29248, 'glamorous': 29249, 'sparingly': 29250, 'blushing': 29251, 'hamburg': 29252, 'endure': 29253, 'jeered': 29254, 'birkbeck': 29255, 'barracking': 29256, 'follet': 29257, 'babes': 29258, 'wiggle': 29259, 'teather': 29260, 'exaggeration': 29261, 'gratified': 29262, 'croke': 29263, 'mcd': 29264, 'ticketmaster': 29265, 'firming': 29266, 'sagging': 29267, 'goodwin': 29268, 'gent': 29269, 'masochism': 29270, 'unwashed': 29271, 'freakery': 29272, 'flagellation': 29273, 'marching': 29274, 'grilling': 29275, 'interviewer': 29276, 'coppendale': 29277, 'granger': 29278, 'questioners': 29279, 'sharron': 29280, 'storer': 29281, 'harangued': 29282, 'stomping': 29283, 'punched': 29284, 'belgrano': 29285, 'falklands': 29286, 'soapbox': 29287, 'eggs': 29288, 'memoirs': 29289, 'artful': 29290, 'enthuse': 29291, 'maxim': 29292, 'henley': 29293, 'perpetuating': 29294, 'wallowing': 29295, 'grovelling': 29296, 'thuggish': 29297, 'dubliner': 29298, 'eastlands': 29299, 'dishevelled': 29300, 'outfield': 29301, '126': 29302, '짙178': 29303, '906': 29304, '2944': 29305, '2100gmt': 29306, '3006': 29307, '166bn': 29308, 'carsten': 29309, 'fritsch': 29310, 'commerzbank': 29311, '짙64m': 29312, 'harbin': 29313, 'frontrunners': 29314, '190bn': 29315, 'xuebing': 29316, 'worthiness': 29317, 'wen': 29318, 'jiabao': 29319, 'codename': 29320, 'leaps': 29321, 'horsepower': 29322, 'pixel': 29323, 'shaders': 29324, 'canvas': 29325, 'filmic': 29326, 'indistinguishable': 29327, 'tryscorers': 29328, 'economical': 29329, '7e7s': 29330, 'fido': 29331, 'identifiers': 29332, 'inaccuracies': 29333, 'redirection': 29334, 'retrace': 29335, 'residences': 29336, 'warden': 29337, 'caucaunibuca': 29338, 'rawaqa': 29339, 'reihana': 29340, 'unfrozen': 29341, 'libyan': 29342, 'vowing': 29343, 'farhat': 29344, 'gadaravice': 29345, 'unfreeze': 29346, '짙347m': 29347, '짙801m': 29348, '1405': 29349, 'celts': 29350, 'savour': 29351, 'heady': 29352, 'ignominy': 29353, 'phalanx': 29354, 'jpr': 29355, 'tenterhooks': 29356, 'blossoming': 29357, 'dawns': 29358, 'emerald': 29359, 'scalps': 29360, 'romantics': 29361, 'legions': 29362, 'bleus': 29363, 'abtahi': 29364, 'wikinews': 29365, 'monde': 29366, 'linguist': 29367, 'chomsky': 29368, 'acclimate': 29369, 'discourse': 29370, 'exponential': 29371, 'movabletype': 29372, 'awakening': 29373, 'untouchables': 29374, 'stared': 29375, 'joanna': 29376, '1936': 29377, 'boyzone': 29378, 'gibb': 29379, 'olivia': 29380, 'barbados': 29381, 'enormity': 29382, 'suddenness': 29383, 'aimlessly': 29384, '짙266m': 29385, 'coordinate': 29386, 'inaugurated': 29387, 'telecentre': 29388, '2kw': 29389, 'strive': 29390, 'margherita': 29391, 'undimmed': 29392, 'blurry': 29393, 'sharpness': 29394, 'widescreen': 29395, 'telly': 29396, 'flaherty': 29397, 'euro1080': 29398, 'salterbaxter': 29399, 'harwood': 29400, 'greenpeace': 29401, 'verified': 29402, 'lexicon': 29403, 'homilies': 29404, 'generalities': 29405, 'sodomy': 29406, 'donoghue': 29407, 'crowning': 29408, 'agreeable': 29409, 'rubbed': 29410, 'manor': 29411, 'buffers': 29412, 'splashed': 29413, 'pragmatist': 29414, 'irreplaceable': 29415, 'milkman': 29416, 'invade': 29417, 'allocation': 29418, 'evanescence': 29419, 'trevin': 29420, 'censored': 29421, 'brownsville': 29422, '660': 29423, 'marts': 29424, 'hagerstown': 29425, 'overdraft': 29426, 'unshredded': 29427, 'cratchit': 29428, 'underworld': 29429, 'auditing': 29430, 'plugging': 29431, 'securewave': 29432, 'transit': 29433, 'convoluted': 29434, 'calculators': 29435, 'eclipse': 29436, 'becker': 29437, 'harel': 29438, 'okun': 29439, 'outwards': 29440, 'dogville': 29441, 'lars': 29442, 'trier': 29443, '짙266': 29444, '582': 29445, 'profanities': 29446, 'revoke': 29447, 'superbowl': 29448, '264': 29449, 'legisation': 29450, 'politcians': 29451, 'brownbeck': 29452, '320': 29453, 'adjourned': 29454, 'tabling': 29455, 'fiduciary': 29456, 'unconscionable': 29457, 'unhappiness': 29458, 'capellas': 29459, 'transcription': 29460, 'kung': 29461, 'fu': 29462, 'nixon': 29463, 'hermann': 29464, 'szilard': 29465, 'mopped': 29466, '74th': 29467, 'capitulate': 29468, 'reiziger': 29469, 'doriva': 29470, 'parnaby': 29471, 'euell': 29472, 'kishishev': 29473, 'jeffers': 29474, 'igels': 29475, 'nordic': 29476, 'teliasonera': 29477, 'conditional': 29478, 'shin': 29479, 'watchable': 29480, 'yeob': 29481, 'yeun': 29482, 'negotiates': 29483, 'radioscape': 29484, 'wantage': 29485, 'unilateral': 29486, 'denunciation': 29487, 'rightwards': 29488, 'defect': 29489, 'macedonian': 29490, '912km': 29491, 'burgas': 29492, 'vlore': 29493, 'balkans': 29494, 'fatos': 29495, 'opic': 29496, 'eximbank': 29497, 'mobil': 29498, 'bosphorus': 29499, 'dardanelles': 29500, 'straits': 29501, 'unwavering': 29502, 'bleeps': 29503, 'disapproved': 29504, 'humorously': 29505, 'vicissitudes': 29506, 'sarin': 29507, 'counterterrorism': 29508, 'visibility': 29509, 'erected': 29510, 'canopy': 29511, 'receded': 29512, 'mink': 29513, 'eyelashes': 29514, 'hindering': 29515, '짙680': 29516, 'orkneys': 29517, 'benchmarks': 29518, 'speedily': 29519, 'sweetest': 29520, 'excites': 29521, 'skillset': 29522, 'underpinned': 29523, 'caption': 29524, 'rosenberg': 29525, 'span': 29526, 'webcasts': 29527, 'captioning': 29528, 'painter': 29529, 'della': 29530, 'passages': 29531, 'telemetric': 29532, 'businessmen': 29533, 'yoshiaki': 29534, 'untouchable': 29535, 'inheriting': 29536, 'nagano': 29537, 'rebuilt': 29538, 'dined': 29539, 'scolded': 29540, 'soy': 29541, 'sauce': 29542, 'futon': 29543, 'schumer': 29544, 'vermont': 29545, 'tracy': 29546, 'schmaler': 29547, '3516': 29548, '3509': 29549, 'coordinated': 29550, 'abnormal': 29551, 'upturns': 29552, 'parente': 29553, 'prising': 29554, 'respecter': 29555, 'ejection': 29556, 'brookings': 29557, 'pharmacies': 29558, 'bechtel': 29559, 'halliburton': 29560, 'uneasily': 29561, 'multinationals': 29562, 'baathist': 29563, 'exceptions': 29564, 'kirkuk': 29565, 'geo': 29566, 'malls': 29567, 'lynn': 29568, 'threefold': 29569, '186': 29570, '짙97m': 29571, 'nonethless': 29572, 'hostilities': 29573, 'plastics': 29574, 'aberration': 29575, 'revelled': 29576, 'santiago': 29577, 'solari': 29578, 'helguera': 29579, 'doblas': 29580, 'waggoner': 29581, 'attaining': 29582, 'keenness': 29583, 'sexing': 29584, '0950': 29585, '3218': 29586, '9094': 29587, 'steadied': 29588, '850bn': 29589, '142m': 29590, '935': 29591, '648': 29592, 'janco': 29593, 'pyykkonen': 29594, '778': 29595, 'meg': 29596, 'whitman': 29597, 'terabyte': 29598, 'lossless': 29599, 'encrypted': 29600, 'scratches': 29601, '2k': 29602, '1080': 29603, 'roaring': 29604, 'cowardly': 29605, 'unbelievably': 29606, 'rudolf': 29607, 'outgrowth': 29608, 'ralph': 29609, 'leftwing': 29610, 'leaning': 29611, 'haverstock': 29612, 'msc': 29613, 'rebuilds': 29614, 'starfox': 29615, 'outgunned': 29616, 'superclubs': 29617, 'consciously': 29618, 'subconsciously': 29619, 'scrub': 29620, 'valium': 29621, 'pacing': 29622, 'revellers': 29623, 'horizons': 29624, 'landlocked': 29625, 'maracana': 29626, 'peppe': 29627, 'jolly': 29628, 'accumulates': 29629, 'rattling': 29630, 'displacing': 29631, 'spanglish': 29632, 'connelly': 29633, 'entertained': 29634, 'whisked': 29635, 'handler': 29636, 'olaf': 29637, 'counterproductive': 29638, 'braved': 29639, 'boasted': 29640, 'waltzing': 29641, 'ballroom': 29642, 'chelsom': 29643, 'swollen': 29644, 'glands': 29645, 'lowell': 29646, 'dobson': 29647, 'apologising': 29648, 'bluewater': 29649, 'enoch': 29650, 'walton': 29651, 'mobbed': 29652, 'footfall': 29653, 'heaving': 29654, 'beale': 29655, 'leno': 29656, 'blige': 29657, 'uma': 29658, 'thurman': 29659, 'superstars': 29660, 'ditched': 29661, 'beijingers': 29662, 'fume': 29663, 'choking': 29664, 'jams': 29665, 'reorganising': 29666, 'clogged': 29667, 'circling': 29668, 'yan': 29669, 'bays': 29670, 'jacking': 29671, 'motorcades': 29672, 'outriders': 29673, 'unclogging': 29674, 'impassable': 29675, 'grocery': 29676, 'parul': 29677, '짙143': 29678, 'tent': 29679, 'balloch': 29680, 'lomond': 29681, 'hampden': 29682, 'bellahouston': 29683, 'ticketweb': 29684, 'snowball': 29685, 'rationally': 29686, 'wonks': 29687, 'heinous': 29688, 'convict': 29689, 'mouths': 29690, 'induce': 29691, '283': 29692, 'selfishly': 29693, 'ensues': 29694, 'perverse': 29695, 'exhorting': 29696, 'solomon': 29697, 'allocating': 29698, 'heerenveen': 29699}\n",
      "['OOV']\n",
      "['along']\n",
      "[[1259]]\n"
     ]
    }
   ],
   "source": [
    "#기사 Tokenization 처리 \n",
    "\n",
    "A_token = Tokenizer(num_words=MY_VOCAB,\n",
    "                    oov_token='OOV')\n",
    "print(A_token.word_index)\n",
    "A_token.fit_on_texts(articles)\n",
    "print(A_token.word_index)\n",
    "\n",
    "#숫자를 단어로 전환 \n",
    "#sequence: 숫자, text: 단어 \n",
    "#OOV: out of vocabulary 특수 문자 \n",
    "print(A_token.sequences_to_texts([[1]]))\n",
    "print(A_token.sequences_to_texts([[1012]]))\n",
    "\n",
    "print(A_token.texts_to_sequences(['the']))\n",
    "\n",
    "# 모든 제외어를 뺀 기사를 tokenizing\n",
    "A_tokenized= A_token.texts_to_sequences(articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1754, 1, 689, 2077, 33, 884, 118, 1041, 76, 1754, 1, 2, 689, 34, 49, 10, 67, 2831, 525, 1273, 3008, 3335, 213, 3791, 1, 33, 1, 336, 1324, 3, 1, 293, 232, 388, 67, 1041, 1, 3062, 2690, 464, 1, 62, 4805, 3, 67, 2, 430, 972, 33, 4183, 338, 2303, 33, 119, 718, 953, 3, 1, 314, 39, 1097, 2440, 140, 223, 1260, 1424, 334, 178, 3791, 76, 1560, 2237, 809, 733, 1, 1, 2882, 1275, 1, 1626, 17, 1602, 265, 19, 2872, 380, 875, 53, 502, 834, 1326, 1, 1, 17, 1]\n제일 긴 기사 단어 수: 2279\n제일 짧은 기사 단어 수: 50\n"
     ]
    }
   ],
   "source": [
    "#tokenizing(Text-2-Sequence)) Output -> 각 word를 정수로 \n",
    "sample = A_tokenized[MY_SAMPLE]\n",
    "print(sample)\n",
    "\n",
    "#기사 통계 내기 \n",
    "longest = max([len(x) for x in A_tokenized])\n",
    "print('제일 긴 기사 단어 수:', longest)\n",
    "\n",
    "shortest = min([len(x) for x in A_tokenized])\n",
    "print('제일 짧은 기사 단어 수:', shortest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "제일 긴 기사 단어 수: 200\n제일 짧은 기사 단어 수: 200\nhash map에 사용된 총 단어 수: 29698\n최종 처리된 샘플 기사\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0 1754    1  689 2077   33  884  118\n 1041   76 1754    1    2  689   34   49   10   67 2831  525 1273 3008\n 3335  213 3791    1   33    1  336 1324    3    1  293  232  388   67\n 1041    1 3062 2690  464    1   62 4805    3   67    2  430  972   33\n 4183  338 2303   33  119  718  953    3    1  314   39 1097 2440  140\n  223 1260 1424  334  178 3791   76 1560 2237  809  733    1    1 2882\n 1275    1 1626   17 1602  265   19 2872  380  875   53  502  834 1326\n    1    1   17    1]\n"
     ]
    }
   ],
   "source": [
    "#기사의 길이 모두 맞추기 \n",
    "\n",
    "#padding: 200개보다 짧은 경우 처리\n",
    "#pad_sequence 결과는 numpy로 처리\n",
    "A_tokenized = pad_sequences(A_tokenized,\n",
    "                            maxlen=MY_LEN,\n",
    "                            padding='pre',\n",
    "                            truncating='pre')\n",
    "\n",
    "\n",
    "# 기사 통계 내기\n",
    "longest = max([len(x) for x in A_tokenized])\n",
    "print('제일 긴 기사 단어 수:', longest)\n",
    "\n",
    "shortest = min([len(x) for x in A_tokenized])\n",
    "print('제일 짧은 기사 단어 수:', shortest)\n",
    "\n",
    "print('hash map에 사용된 총 단어 수:', len(A_token.word_counts))\n",
    "\n",
    "print('최종 처리된 샘플 기사')\n",
    "print(A_tokenized[MY_SAMPLE])\n",
    "\n",
    "\n",
    "#95개의 단어를 가졌기 때문에 앞부분(PRE)에 105개의 Zero padding이 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "', 'tech', 'politics', 'business', 'sport', 'politics', 'entertainment', 'tech', 'entertainment', 'sport', 'tech', 'business', 'politics', 'entertainment', 'tech', 'politics', 'sport', 'sport', 'business', 'entertainment', 'business', 'business', 'politics', 'business', 'business', 'politics', 'tech', 'entertainment', 'politics', 'entertainment', 'entertainment', 'business', 'politics', 'sport', 'politics', 'entertainment', 'sport', 'entertainment', 'politics', 'entertainment', 'entertainment', 'entertainment', 'business', 'sport', 'politics', 'sport', 'entertainment', 'tech', 'tech', 'business', 'politics', 'entertainment', 'business', 'sport', 'business', 'politics', 'politics', 'business', 'sport', 'tech', 'tech', 'tech', 'business', 'politics', 'politics', 'business', 'sport', 'sport', 'entertainment', 'business', 'sport', 'sport', 'politics', 'politics', 'politics', 'entertainment', 'politics', 'business', 'tech', 'business', 'politics', 'entertainment', 'entertainment', 'business', 'sport', 'politics', 'entertainment', 'sport', 'entertainment', 'politics', 'politics', 'business', 'politics', 'sport', 'entertainment', 'entertainment', 'tech', 'tech', 'tech', 'tech', 'politics', 'tech', 'business', 'tech', 'sport', 'sport', 'entertainment', 'business', 'tech', 'entertainment', 'business', 'tech', 'sport', 'tech', 'entertainment', 'entertainment', 'politics', 'politics', 'sport', 'business', 'business', 'tech', 'entertainment', 'sport', 'politics', 'business', 'tech', 'entertainment', 'politics', 'sport', 'entertainment', 'politics', 'politics', 'tech', 'sport', 'politics', 'business', 'politics', 'entertainment', 'tech', 'sport', 'politics', 'entertainment', 'entertainment', 'business', 'sport', 'tech', 'tech', 'sport', 'entertainment', 'tech', 'politics', 'tech', 'sport', 'politics', 'politics', 'sport', 'entertainment', 'sport', 'politics', 'sport', 'tech', 'entertainment', 'sport', 'entertainment', 'business', 'business', 'entertainment', 'entertainment', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'politics', 'business', 'politics', 'entertainment', 'sport', 'sport', 'tech', 'politics', 'sport', 'business', 'sport', 'business', 'tech', 'sport', 'sport', 'business', 'entertainment', 'sport', 'sport', 'tech', 'sport', 'business', 'politics', 'business', 'entertainment', 'tech', 'entertainment', 'politics', 'business', 'entertainment', 'politics', 'entertainment', 'politics', 'politics', 'business', 'sport', 'sport', 'sport', 'tech', 'tech', 'sport', 'politics', 'entertainment', 'tech', 'sport', 'entertainment', 'business', 'entertainment', 'politics', 'business', 'sport', 'tech', 'business', 'sport', 'politics', 'business', 'entertainment', 'entertainment', 'entertainment', 'politics', 'entertainment', 'politics', 'entertainment', 'sport', 'entertainment', 'sport', 'politics', 'politics', 'business', 'politics', 'tech', 'sport', 'tech', 'sport', 'business', 'politics', 'tech', 'entertainment', 'entertainment', 'politics', 'entertainment', 'politics', 'sport', 'sport', 'politics', 'business', 'tech', 'sport', 'politics', 'politics', 'entertainment', 'sport', 'politics', 'politics', 'business', 'tech', 'tech', 'business', 'tech', 'sport', 'sport', 'business', 'politics', 'business', 'tech', 'sport', 'tech', 'politics', 'entertainment', 'sport', 'business', 'sport', 'entertainment', 'tech', 'tech', 'sport', 'politics', 'sport', 'business', 'sport', 'business', 'sport', 'sport', 'entertainment', 'entertainment', 'business', 'tech', 'tech', 'business', 'tech', 'business', 'business', 'sport', 'sport', 'politics', 'sport', 'tech', 'sport', 'tech', 'sport', 'sport', 'business', 'business', 'tech', 'sport', 'business', 'tech', 'tech', 'politics', 'sport', 'business', 'entertainment', 'entertainment', 'tech', 'politics', 'sport', 'sport', 'tech', 'business', 'tech', 'entertainment', 'entertainment', 'politics', 'politics', 'business', 'entertainment', 'sport', 'entertainment', 'entertainment', 'business', 'sport', 'politics', 'tech', 'business', 'sport', 'entertainment', 'tech', 'politics', 'politics', 'sport', 'sport', 'business', 'business', 'business', 'business', 'business', 'entertainment', 'business', 'politics', 'politics', 'tech', 'tech', 'entertainment', 'politics', 'entertainment', 'business', 'politics', 'sport', 'sport', 'business', 'tech', 'business', 'tech', 'sport', 'business', 'business', 'politics', 'sport', 'sport', 'politics', 'politics', 'entertainment', 'entertainment', 'politics', 'business', 'politics', 'politics', 'business', 'tech', 'politics', 'politics', 'politics', 'politics', 'politics', 'entertainment', 'entertainment', 'tech', 'politics', 'sport', 'politics', 'sport', 'tech', 'sport', 'sport', 'sport', 'entertainment', 'tech', 'entertainment', 'tech', 'sport', 'tech', 'sport', 'sport', 'entertainment', 'entertainment', 'business', 'entertainment', 'entertainment', 'politics', 'business', 'sport', 'tech', 'tech', 'politics', 'business', 'sport', 'business', 'entertainment', 'sport', 'politics', 'entertainment', 'sport', 'sport', 'business', 'politics', 'tech', 'tech', 'business', 'business', 'sport', 'politics', 'entertainment', 'sport', 'sport', 'politics', 'entertainment', 'tech', 'sport', 'entertainment', 'business', 'politics', 'sport', 'politics', 'tech', 'entertainment', 'sport', 'politics', 'business', 'entertainment', 'politics', 'entertainment', 'politics', 'sport', 'politics', 'business', 'business', 'politics', 'business', 'tech', 'sport', 'business', 'entertainment', 'business', 'sport', 'sport', 'entertainment', 'tech', 'politics', 'tech', 'politics', 'politics', 'sport', 'sport', 'sport', 'business', 'tech', 'entertainment', 'business', 'sport', 'business', 'sport', 'business', 'tech', 'business', 'business', 'business', 'business', 'sport', 'sport', 'sport', 'business', 'business', 'entertainment', 'business', 'business', 'business', 'sport', 'sport', 'sport', 'tech', 'business', 'sport', 'sport', 'sport', 'business', 'entertainment', 'tech', 'tech', 'tech', 'politics', 'business', 'business', 'tech', 'business', 'tech', 'sport', 'sport', 'politics', 'politics', 'politics', 'sport', 'sport', 'politics', 'politics', 'tech', 'business', 'sport', 'sport', 'sport', 'sport', 'business', 'sport', 'business', 'business', 'business', 'business', 'sport', 'politics', 'sport', 'tech', 'tech', 'sport', 'tech', 'business', 'entertainment', 'business', 'sport', 'business', 'tech', 'tech', 'politics', 'entertainment', 'business', 'entertainment', 'business', 'tech', 'politics', 'entertainment', 'politics', 'entertainment', 'entertainment', 'tech', 'business', 'tech', 'tech', 'business', 'politics', 'sport', 'entertainment', 'business', 'politics', 'entertainment', 'business', 'tech', 'sport', 'tech', 'politics', 'sport', 'politics', 'business', 'politics', 'politics', 'business', 'business', 'business', 'tech', 'politics', 'business', 'politics', 'business', 'entertainment', 'politics', 'sport', 'politics', 'politics', 'sport', 'entertainment', 'tech', 'tech', 'business', 'tech', 'sport', 'business', 'business', 'politics', 'sport', 'tech', 'tech', 'tech', 'sport', 'entertainment', 'sport', 'politics', 'business', 'politics', 'politics', 'entertainment', 'tech', 'business', 'tech', 'sport', 'entertainment', 'entertainment', 'sport', 'business', 'entertainment', 'tech', 'tech', 'sport', 'entertainment', 'business', 'politics', 'politics', 'politics', 'tech', 'tech', 'business', 'politics', 'business', 'politics', 'politics', 'entertainment', 'sport', 'tech', 'business', 'tech', 'entertainment', 'tech', 'tech', 'sport', 'politics', 'sport', 'sport', 'politics', 'entertainment', 'entertainment', 'business', 'politics', 'politics', 'sport', 'business', 'business', 'tech', 'business', 'tech', 'tech', 'sport', 'business', 'politics', 'sport', 'business', 'business', 'sport', 'tech', 'sport', 'tech', 'entertainment', 'sport', 'sport', 'entertainment', 'business', 'tech', 'sport', 'sport', 'politics', 'tech', 'business', 'politics', 'sport', 'sport', 'politics', 'entertainment', 'tech', 'tech', 'tech', 'business', 'sport', 'entertainment', 'entertainment', 'tech', 'business', 'politics', 'tech', 'sport', 'sport', 'tech', 'tech', 'politics', 'business', 'sport', 'entertainment', 'entertainment', 'sport', 'entertainment', 'tech', 'tech', 'business', 'business', 'business', 'sport', 'sport', 'tech', 'sport', 'business', 'tech', 'business', 'business', 'sport', 'business', 'politics', 'business', 'business', 'tech', 'business', 'tech', 'politics', 'tech', 'entertainment', 'politics', 'tech', 'entertainment', 'sport', 'politics', 'entertainment', 'business', 'tech', 'business', 'tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport']\n[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n"
     ]
    }
   ],
   "source": [
    "#'라벨' tokenization 처리 \n",
    "C_token = Tokenizer()\n",
    "C_token.fit_on_texts(labels)\n",
    "print(labels)\n",
    "\n",
    "#단어를 숫자로 전환\n",
    "#texts_to_sequences -> list output\n",
    "C_tokenized = C_token.texts_to_sequences(labels)\n",
    "print(C_tokenized)\n",
    "print(C_token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "학습용 입력 데이터 shape: (1780, 200)\n학습용 출력 데이터 shape: (1780, 1)\n평가용 입력 데이터 shape: (445, 200)\n평가용 출력 데이터 shape: (445, 1)\nMY_SAMPLE =  121\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0 1754    1  689 2077   33  884  118\n 1041   76 1754    1    2  689   34   49   10   67 2831  525 1273 3008\n 3335  213 3791    1   33    1  336 1324    3    1  293  232  388   67\n 1041    1 3062 2690  464    1   62 4805    3   67    2  430  972   33\n 4183  338 2303   33  119  718  953    3    1  314   39 1097 2440  140\n  223 1260 1424  334  178 3791   76 1560 2237  809  733    1    1 2882\n 1275    1 1626   17 1602  265   19 2872  380  875   53  502  834 1326\n    1    1   17    1]\n[3]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    1   38 4411    1 1943   38 2439  569 1386 1482 2973    1    1\n  189    1  193 4569  225 1731 3323    1  213  662 4692 1209 1658 1160\n  227  269  366 1992    1 1749 1524  118 1943 2973    1    1    1 2068\n    1   71  256 1943    1   32 1868  487    1    1  141    1 2439  156\n 1339 3805 2145 1158    5  103 1193 1943 1189  226    1  894 3038  461\n  291   94 1523    1  247   69  298  214  188   26    1    1    1  142\n  264  282    1    1   15  549    1 4131  662 4692 1209 1658  175  366\n  926 2554    1    1    1    1    6   74   28   96 2364    2   12  134\n 1943   22  339  227    8  168    1   14    1    1  366   27  102 2854\n  102  109 1670  168]\n[1]\n"
     ]
    }
   ],
   "source": [
    "#데이터 4분할 \n",
    "#print(type(A_tokenized))\n",
    "#print(type(C_tokenized))\n",
    "\n",
    "#list를 numpy로 전환\n",
    "C_tokenized = np.array(C_tokenized)\n",
    "#print(type(C_tokenized))\n",
    "\n",
    "\n",
    "# 4분할 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized,\n",
    "                                                    C_tokenized,\n",
    "                                                    train_size=MY_SPLIT,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "# 데이터 shape 확인\n",
    "print('학습용 입력 데이터 shape:', X_train.shape)\n",
    "print('학습용 출력 데이터 shape:', Y_train.shape)\n",
    "\n",
    "print('평가용 입력 데이터 shape:', X_test.shape)\n",
    "print('평가용 출력 데이터 shape:', Y_test.shape)\n",
    "\n",
    "# 샘플 출력\n",
    "print(\"MY_SAMPLE = \",MY_SAMPLE)\n",
    "print(X_train[MY_SAMPLE])\n",
    "print(Y_train[MY_SAMPLE])\n",
    "print(X_test[MY_SAMPLE])\n",
    "print(Y_test[MY_SAMPLE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 구현 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MY_VOCAB: 5000\n",
      "MY_EMBED: 64\n",
      "MY_HIDDEN: 100\n",
      "RNN 요약\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 64)          320000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 386,606\n",
      "Trainable params: 386,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#RNN 구현 \n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "print('MY_VOCAB:',MY_VOCAB) #5000\n",
    "print('MY_EMBED:',MY_EMBED) #64\n",
    "print('MY_HIDDEN:',MY_HIDDEN) #100\n",
    "\n",
    "model.add(layers.Embedding(input_dim=MY_VOCAB,\n",
    "                    output_dim=MY_EMBED))\n",
    "\n",
    "model.add(layers.Dropout(rate=0.5))\n",
    "\n",
    "#this is the secret sauce!\n",
    "model.add(layers.LSTM(units=MY_HIDDEN))\n",
    "\n",
    "model.add(layers.Dense(units=6, activation='softmax'))\n",
    "\n",
    "\n",
    "print('RNN 요약')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#인공신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MY_EPOCH: 10\n",
      "학습 시작\n",
      "Epoch 1/10\n",
      "C:\\Users\\PC\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "CancelledError",
     "evalue": " [_Derived_]RecvAsync is cancelled.\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape/_30}}]] [Op:__inference_train_function_18255]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-821ea5bb66ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'학습 시작'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mbegin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m model.fit(X_train,\n\u001b[0m\u001b[0;32m     12\u001b[0m           \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMY_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m:  [_Derived_]RecvAsync is cancelled.\n\t [[{{node gradient_tape/sequential_4/embedding_4/embedding_lookup/Reshape/_30}}]] [Op:__inference_train_function_18255]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "#RNN 학습 환경 설정 \n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "                ,metrics=['acc'])\n",
    "\n",
    "\n",
    "print('MY_EPOCH:',MY_EPOCH)\n",
    "print('학습 시작')\n",
    "begin = time()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          epochs=MY_EPOCH,\n",
    "          verbose=1)re\n",
    "end = time()\n",
    "\n",
    "print('총 학습 시간: {:.2f}'.format(end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}